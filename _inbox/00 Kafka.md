---
aliases:
  - кафка
  - kafka
tags:
  - type/moc
date:
  - - 2024-07-02
  - - 2024-07-02
zero-link:
  - "[[00 Архитектура ПО]]"
linked:
---
Apache Kafka – это платформа потоковой передачи, которую можно использовать для быстрой обработки большого количества событий. Строго говоря это не [Брокер сообщений](Брокер%20сообщений.md), а распределенный реплицированный журнал фиксаций изменений (commit log).

Основные возможности:
- Публиковать записи и подписываться на очереди сообщений (топики)
- Хранить записи с отказоустойчивостью
- Обрабатывать поток по мере их возникновения

Где встречается потребности в потоковой обработке:
- Финансовые транзакции
- Аналитика
	- Действия в онлайн магазинах
	- Перемещения пользователей
- Социальные сети
- Промышленные датчики

Примеры использований:
- Буфферизация.
	- Агрегация журналов логов. 
- Интернет вещей
	- Показатели датчиков
- [CQRS](CQRS.md)

![](Pasted%20image%2020240703120512.png)

Характеристики:
- Гарантия доставки
- Репликация журнала и синхронизация реплик
- Уведомление о смещении (offset) получателя
- Уведомление о получении сообщения
- Данные хранятся в виде файлов. Файлы записываются в виде сегментов.
- Хранит свои записи на диске и не использует RAM. Использует кэш диска
- Операции считывания и записи выполняются за константное время.
- Реализует последовательное чтение и запись на диск
- Работает через pull модель. Consumer сам должен опрашивать кафку.

Платформа:
- API
	- Producer
	- Consumer
	- Admin
- [Kafka Connect](Kafka%20Connect.md)
- [Kafka Stream](Kafka%20Stream.md)
- ksqlDB 

В отличии от [RabbitMQ](00%20RabbitMQ.md) кафка не реализует какой-то роутинг внутри. Предполагается, что за это отвечает consumer.

Терминология:
- Record - запись состоящая из ключа и значения
- [Topic](Kafka%20Topic.md) - имя потока, куда публикуются Record
- Offset - позиция записи
- Pratition - шард топика




Доставка сообщений в Kafka может осуществляться как минимум тремя способами:
- не менее одного раза (at-least-once) – сообщение будет отправляется потребителям до тех пор, пока те не подтвердят его получение;
- не более одного раза (at-most-once) – сообщение отправляется только один раз и в случае сбоя не отправляется повторно;
- точно один раз (exactly-once) – потребитель гарантированно получит сообщение ровно один раз.

![](c85390a0-94b2-48f9-8dea-02d9bacc2562.jpg)

![](49356685-5474-490b-975a-d198d7f966bb.jpg)

![](3ad2bf28-e12a-4be7-9336-c38c7b5d7018.jpg)

Помимо различных семантик доставки, есть еще одно общее преимущество использования брокера сообщений – если приложение потребитель потерпело аварию или остановлено для технического обслуживания, то производитель может не ждать, пока его сообщение будет обработано. Когда потребители возобновят работу и вернутся в сеть, они смогут продолжить с того места, на котором остановились, и обработать ожидающие сообщения.

Платформа Kafka изначально была ориентирована на работу с несколькими потребителями, - модель Publish & Subscribe. Это означает, что приложение, читающее сообщение из брокера сообщений, не делает это сообщение недоступным для других приложений, которые также могут захотеть его получить и использовать. Consumer сам следит за смещением offset, то есть он сам следит за тем, какие сообщения он уже прочитал.

Сообщение, также называемое записью, является основной частью данных, проходящих через Kafka. Сообщения – это представление ваших данных в Kafka. Каждое сообщение имеет отметку времени, значение и необязательный ключ. При желании также можно добавлять свои заголовки

![](ae8f6a39-5a3f-4195-8f6a-0b96e492413c.jpg)

## Заметки
- Реализована на Java
- Не использует RAM для доступа к данным.
- Зачем нужен Zookeper
	- Чтобы определить какая из нод лидер для конкретного топика
	- Какое положение курсоров
	- Где находятся партишены 
- Отправленные сообщения не удаляются после прочтения. 
	- Позволяет повторно считать данные с любого места. Например, мы выполняли какую-то бухгалтерскую логику и поняли, что в приложении была допущена ошибка в формуле расчета. Мы можем удалить невалидные данные, поправить формулу и заново считать все старые сообщения.
	- Можно настроить ротацию, чтобы старые сообщения удалялись.
- Для улучшения производительности желательно под кафку выделять отдельные жесткие диски. Менее актуально для SSD.
- Есть возможность сжатия данных в топике. Аля архивация.
- Использует zookeeper для хранения мета-данных. В новых версиях может уже не использовать.
	- Хранит offset значения продюсеров.
- Может обработать 30000 сообщений в секунду
- Производительнее, чем [RabbitMQ](00%20RabbitMQ.md)

## Дополнительно
- [Consumer Group](Consumer%20Group.md)
- [[Кластер Kafka]]
- [Запуск Kafka в docker-compose](Запуск%20Kafka%20в%20docker-compose.md)

```
sudo docker run -p 9000:9000 -e ZK_HOSTS="10.21.21.14:2181" sheepkiller/kafka-manager
```